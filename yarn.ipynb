{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/miniconda/envs/dfs/lib/python3.8/site-packages/dask_yarn/core.py:16: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import (\n",
      "/home/hadoop/miniconda/envs/dfs/lib/python3.8/site-packages/dask_yarn/core.py:16: FutureWarning: parse_timedelta is deprecated and will be removed in a future release. Please use dask.utils.parse_timedelta instead.\n",
      "  from distributed.utils import (\n"
     ]
    }
   ],
   "source": [
    "from dask_yarn import YarnCluster\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "os.environ['CRYPTOGRAPHY_OPENSSL_NO_LEGACY'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 05:24:03,453 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2023-09-22 05:24:03,455 - distributed.scheduler - INFO - State start\n",
      "2023-09-22 05:24:03,457 - distributed.scheduler - INFO -   Scheduler at: tcp://172.31.36.178:40869\n",
      "2023-09-22 05:24:03,458 - distributed.scheduler - INFO -   dashboard at:  http://172.31.36.178:44379/status\n",
      "23/09/22 05:24:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/22 05:24:04 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-36-178.ap-southeast-1.compute.internal/172.31.36.178:8032\n",
      "23/09/22 05:24:04 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-36-178.ap-southeast-1.compute.internal/172.31.36.178:10200\n",
      "23/09/22 05:24:05 INFO skein.Driver: Driver started, listening on 40353\n",
      "23/09/22 05:24:05 INFO conf.Configuration: resource-types.xml not found\n",
      "23/09/22 05:24:05 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "23/09/22 05:24:05 INFO skein.Driver: Uploading application resources to hdfs://ip-172-31-36-178.ap-southeast-1.compute.internal:8020/user/hadoop/.skein/application_1695350354974_0010\n",
      "23/09/22 05:24:06 INFO skein.Driver: Submitting application...\n",
      "23/09/22 05:24:06 INFO impl.YarnClientImpl: Submitted application application_1695350354974_0010\n"
     ]
    }
   ],
   "source": [
    "cluster = YarnCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 05:24:10,933 - distributed.scheduler - INFO - Receive client connection: Client-429b732b-5908-11ee-8025-06d7787525d8\n",
      "2023-09-22 05:24:10,935 - distributed.core - INFO - Starting established connection to tcp://172.31.36.178:50520\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster.scheduler_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Container<service_name='dask.worker', instance=0, state=RUNNING>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chain', 'offer', 'market', 'repeater', 'offerdate', 'id'], dtype='object')\n",
      "Index(['index', 'id', 'chain', 'dept', 'category', 'company', 'brand', 'date',\n",
      "       'productsize', 'productmeasure', 'purchasequantity', 'purchaseamount'],\n",
      "      dtype='object')\n",
      "Index(['offer', 'category', 'quantity', 'company', 'offervalue', 'brand'], dtype='object')\n",
      "Built 30 features\n",
      "<Feature: chain>\n",
      "<Feature: offer>\n",
      "<Feature: market>\n",
      "<Feature: repeater>\n",
      "<Feature: COUNT(transactions)>\n",
      "<Feature: MAX(transactions.productsize)>\n",
      "<Feature: MAX(transactions.purchaseamount)>\n",
      "<Feature: MAX(transactions.purchasequantity)>\n",
      "<Feature: MEAN(transactions.productsize)>\n",
      "<Feature: MEAN(transactions.purchaseamount)>\n",
      "<Feature: MEAN(transactions.purchasequantity)>\n",
      "<Feature: MIN(transactions.productsize)>\n",
      "<Feature: MIN(transactions.purchaseamount)>\n",
      "<Feature: MIN(transactions.purchasequantity)>\n",
      "<Feature: offers.category>\n",
      "<Feature: offers.quantity>\n",
      "<Feature: offers.company>\n",
      "<Feature: offers.offervalue>\n",
      "<Feature: offers.brand>\n",
      "<Feature: offers.COUNT(history)>\n",
      "<Feature: offers.COUNT(transactions)>\n",
      "<Feature: offers.MAX(transactions.productsize)>\n",
      "<Feature: offers.MAX(transactions.purchaseamount)>\n",
      "<Feature: offers.MAX(transactions.purchasequantity)>\n",
      "<Feature: offers.MEAN(transactions.productsize)>\n",
      "<Feature: offers.MEAN(transactions.purchaseamount)>\n",
      "<Feature: offers.MEAN(transactions.purchasequantity)>\n",
      "<Feature: offers.MIN(transactions.productsize)>\n",
      "<Feature: offers.MIN(transactions.purchaseamount)>\n",
      "<Feature: offers.MIN(transactions.purchasequantity)>\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import featuretools as ft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "# %%\n",
    "data_dir = '/home/hadoop/avs/'\n",
    "history = dd.read_parquet(\"s3://dgl-data-private/zymao_dfs/avs/history.parquet/*.parquet\", engine='pyarrow', storage_options={'use_ssl': False})\n",
    "transactions = dd.read_parquet('s3://dgl-data-private/zymao_dfs/avs/transactions.parquet/*.parquet',engine='pyarrow', storage_options={'use_ssl': False})\n",
    "transactions = transactions.reset_index()\n",
    "offers = dd.read_parquet('s3://dgl-data-private/zymao_dfs/avs/offer.parquet/*.parquet',engine='pyarrow', storage_options={'use_ssl': False})\n",
    "\n",
    "# %%\n",
    "history = history[['chain','offer','market','repeater','offerdate','id']]\n",
    "transactions = transactions[['index','id', 'chain',   'dept'   ,'category','company', 'brand'  ,'date','productsize','productmeasure','purchasequantity','purchaseamount']]\n",
    "offers  = offers [[ 'offer' ,   'category',    'quantity' ,     'company' ,   'offervalue','brand'  ]]\n",
    "\n",
    "# %%\n",
    "print(history.columns)\n",
    "print(transactions.columns)\n",
    "print(offers.columns)\n",
    "\n",
    "# %%\n",
    "from woodwork.logical_types import Datetime\n",
    "dateformat=Datetime(datetime_format=\"1970-01-01\")\n",
    "\n",
    "es = ft.EntitySet(id='history')\n",
    "es.add_dataframe(dataframe_name='history',\n",
    "                                dataframe=history,\n",
    "                                index='id',\n",
    "                                logical_types={\n",
    "                                    'id':'Integer',\n",
    "                                    'chain': \"Categorical\",\n",
    "                                                'market': \"Categorical\",\n",
    "                                                'offer': \"Integer\",\n",
    "                                                'offerdate': dateformat,\n",
    "                                                'repeater': \"Categorical\"\n",
    "                                                }\n",
    "                                )\n",
    "es.add_dataframe(dataframe_name='transactions',\n",
    "                                dataframe=transactions,\n",
    "                                index='index',\n",
    "                                logical_types= \n",
    "                                            {\n",
    "                                                'index':'Integer',\n",
    "                                                'id':'Integer',\n",
    "                                                'chain': \"Categorical\",\n",
    "                                                'dept': \"Categorical\",\n",
    "                                                'category': \"Categorical\",\n",
    "                                                'company': \"Categorical\",\n",
    "                                                'brand': \"Categorical\",\n",
    "                                                'date': dateformat,\n",
    "                                                'productsize': \"Double\",\n",
    "                                                'productmeasure':'Categorical',\n",
    "                                                'purchasequantity': \"Double\",\n",
    "                                                'purchaseamount': \"Double\"\n",
    "                                                }\n",
    "                                )\n",
    "\n",
    "es.add_dataframe(dataframe_name='offers',\n",
    "                                dataframe=offers,\n",
    "                                index='offer',\n",
    "                                logical_types= \n",
    "                                            {\n",
    "                                                'offer':'Integer',\n",
    "                                                'category': \"Categorical\",\n",
    "                                                'quantity': \"Double\",\n",
    "                                                'company': \"Categorical\",\n",
    "                                                'offervalue': \"Double\",\n",
    "                                                'brand': \"Categorical\"\n",
    "                                                }\n",
    "                                )\n",
    "# Define relationship\n",
    "es.add_relationship(\n",
    "parent_dataframe_name=\"history\",\n",
    "parent_column_name=\"id\",\n",
    "child_dataframe_name=\"transactions\",\n",
    "child_column_name=\"id\")\n",
    "\n",
    "es.add_relationship(\n",
    "parent_dataframe_name=\"offers\",\n",
    "parent_column_name=\"offer\",\n",
    "child_dataframe_name=\"history\",\n",
    "child_column_name=\"offer\")\n",
    "# # %%\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "import warnings\n",
    "\n",
    "from featuretools.computational_backends import calculate_feature_matrix\n",
    "from featuretools.entityset import EntitySet\n",
    "from featuretools.exceptions import UnusedPrimitiveWarning\n",
    "from featuretools.synthesis.deep_feature_synthesis import DeepFeatureSynthesis\n",
    "from featuretools.synthesis.utils import _categorize_features, get_unused_primitives\n",
    "from featuretools.synthesis.dfs import warn_unused_primitives\n",
    "\n",
    "# %%\n",
    "dataframes=None\n",
    "relationships=None\n",
    "entityset=es\n",
    "target_dataframe_name=\"history\"\n",
    "cutoff_time=None\n",
    "instance_ids=None\n",
    "agg_primitives=['count','mean','min','max']\n",
    "# trans_primitives=['weekday']\n",
    "trans_primitives=[]\n",
    "groupby_trans_primitives=None\n",
    "allowed_paths=None\n",
    "max_depth=2\n",
    "ignore_dataframes=None\n",
    "# ignore_dataframes=[\"pageviews\"]\n",
    "ignore_columns=None\n",
    "primitive_options=None\n",
    "seed_features=None\n",
    "drop_contains=None\n",
    "drop_exact=None\n",
    "where_primitives=None\n",
    "max_features=-1\n",
    "cutoff_time_in_index=False\n",
    "save_progress=None\n",
    "features_only=False\n",
    "training_window=None\n",
    "approximate=None\n",
    "chunk_size=None\n",
    "n_jobs=1\n",
    "dask_kwargs=None\n",
    "verbose=True\n",
    "return_types=None\n",
    "progress_callback=None\n",
    "include_cutoff_time=False\n",
    "\n",
    "# %%\n",
    "dfs_object = DeepFeatureSynthesis(\n",
    "    target_dataframe_name,\n",
    "    entityset,\n",
    "    agg_primitives=agg_primitives,\n",
    "    trans_primitives=trans_primitives,\n",
    "    groupby_trans_primitives=groupby_trans_primitives,\n",
    "    max_depth=max_depth,\n",
    "    where_primitives=where_primitives,\n",
    "    allowed_paths=allowed_paths,\n",
    "    drop_exact=drop_exact,\n",
    "    drop_contains=drop_contains,\n",
    "    ignore_dataframes=ignore_dataframes,\n",
    "    ignore_columns=ignore_columns,\n",
    "    primitive_options=primitive_options,\n",
    "    max_features=max_features,\n",
    "    seed_features=seed_features,\n",
    ")\n",
    "\n",
    "# %%\n",
    "features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n",
    "\n",
    "trans, agg, groupby, where = _categorize_features(features)\n",
    "\n",
    "trans_unused = get_unused_primitives(trans_primitives, trans)\n",
    "agg_unused = get_unused_primitives(agg_primitives, agg)\n",
    "groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n",
    "where_unused = get_unused_primitives(where_primitives, where)\n",
    "\n",
    "unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n",
    "if any(unused_primitives):\n",
    "    warn_unused_primitives(unused_primitives)\n",
    "\n",
    "for feature in features:\n",
    "    print(f\"{feature}\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 05:24:34,569 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.31.42.174:34559', name: dask.worker_0, status: init, memory: 0, processing: 0>\n",
      "2023-09-22 05:24:34,570 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.31.42.174:34559\n",
      "2023-09-22 05:24:34,571 - distributed.core - INFO - Starting established connection to tcp://172.31.42.174:45800\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "OpenSSL 3.0's legacy provider failed to load. This is a fatal error by default, but cryptography supports running without legacy algorithms by setting the environment variable CRYPTOGRAPHY_OPENSSL_NO_LEGACY. If you did not expect this error, you have likely made a mistake with your OpenSSL configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hadoop/DFS2SQL/yarn.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m history\u001b[39m.\u001b[39;49mhead()\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/dask/dataframe/core.py:1391\u001b[0m, in \u001b[0;36m_Frame.head\u001b[0;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39m# No need to warn if we're already looking at all partitions\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m safe \u001b[39m=\u001b[39m npartitions \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnpartitions\n\u001b[0;32m-> 1391\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_head(n\u001b[39m=\u001b[39;49mn, npartitions\u001b[39m=\u001b[39;49mnpartitions, compute\u001b[39m=\u001b[39;49mcompute, safe\u001b[39m=\u001b[39;49msafe)\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/dask/dataframe/core.py:1425\u001b[0m, in \u001b[0;36m_Frame._head\u001b[0;34m(self, n, npartitions, compute, safe)\u001b[0m\n\u001b[1;32m   1420\u001b[0m result \u001b[39m=\u001b[39m new_dd_object(\n\u001b[1;32m   1421\u001b[0m     graph, name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta, [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[npartitions]]\n\u001b[1;32m   1422\u001b[0m )\n\u001b[1;32m   1424\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[0;32m-> 1425\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m   1426\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/client.py:3224\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3222\u001b[0m         should_rejoin \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3224\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgather(packed, asynchronous\u001b[39m=\u001b[39;49masynchronous, direct\u001b[39m=\u001b[39;49mdirect)\n\u001b[1;32m   3225\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   3226\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m futures\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/client.py:2359\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2358\u001b[0m     local_worker \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msync(\n\u001b[1;32m   2360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gather,\n\u001b[1;32m   2361\u001b[0m     futures,\n\u001b[1;32m   2362\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   2363\u001b[0m     direct\u001b[39m=\u001b[39;49mdirect,\n\u001b[1;32m   2364\u001b[0m     local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m   2365\u001b[0m     asynchronous\u001b[39m=\u001b[39;49masynchronous,\n\u001b[1;32m   2366\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/utils.py:351\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[39mreturn\u001b[39;00m future\n\u001b[1;32m    350\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\n\u001b[1;32m    352\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, callback_timeout\u001b[39m=\u001b[39;49mcallback_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    353\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/utils.py:418\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[1;32m    417\u001b[0m     typ, exc, tb \u001b[39m=\u001b[39m error\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/utils.py:391\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m         future \u001b[39m=\u001b[39m wait_for(future, callback_timeout)\n\u001b[1;32m    390\u001b[0m     future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 391\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m future\n\u001b[1;32m    392\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m     error \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/tornado/gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         value \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    768\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    769\u001b[0m         \u001b[39m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[39m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[39m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m         exc: Optional[\u001b[39mException\u001b[39;00m] \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/client.py:2222\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2220\u001b[0m         exc \u001b[39m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2221\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2222\u001b[0m         \u001b[39mraise\u001b[39;00m exception\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2223\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m   2224\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mskip\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/distributed/worker.py:2940\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/distributed/worker.py:2928\u001b[0m, in \u001b[0;36mloads_function\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/distributed/protocol/pickle.py:96\u001b[0m, in \u001b[0;36mloads\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/s3fs/__init__.py:1\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/s3fs/core.py:29\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/aiobotocore/session.py:1\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/botocore/translate.py:16\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/botocore/utils.py:37\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/botocore/httpsession.py:45\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/urllib3/contrib/pyopenssl.py:50\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/OpenSSL/__init__.py:8\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/OpenSSL/SSL.py:9\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/OpenSSL/_util.py:6\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:167\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:134\u001b[0m, in \u001b[0;36minit_static_locks\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:123\u001b[0m, in \u001b[0;36m_ensure_ffi_initialized\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt1/yarn/usercache/hadoop/appcache/application_1695350354974_0010/container_1695350354974_0010_01_000002/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:43\u001b[0m, in \u001b[0;36m_legacy_provider_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: OpenSSL 3.0's legacy provider failed to load. This is a fatal error by default, but cryptography supports running without legacy algorithms by setting the environment variable CRYPTOGRAPHY_OPENSSL_NO_LEGACY. If you did not expect this error, you have likely made a mistake with your OpenSSL configuration."
     ]
    }
   ],
   "source": [
    "history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "OpenSSL 3.0's legacy provider failed to load. This is a fatal error by default, but cryptography supports running without legacy algorithms by setting the environment variable CRYPTOGRAPHY_OPENSSL_NO_LEGACY. If you did not expect this error, you have likely made a mistake with your OpenSSL configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hadoop/DFS2SQL/yarn.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m feature_matrix \u001b[39m=\u001b[39m calculate_feature_matrix(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     features,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     entityset\u001b[39m=\u001b[39;49mentityset,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     cutoff_time\u001b[39m=\u001b[39;49mcutoff_time,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     instance_ids\u001b[39m=\u001b[39;49minstance_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     training_window\u001b[39m=\u001b[39;49mtraining_window,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     approximate\u001b[39m=\u001b[39;49mapproximate,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     cutoff_time_in_index\u001b[39m=\u001b[39;49mcutoff_time_in_index,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     save_progress\u001b[39m=\u001b[39;49msave_progress,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     dask_kwargs\u001b[39m=\u001b[39;49mdask_kwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     progress_callback\u001b[39m=\u001b[39;49mprogress_callback,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     include_cutoff_time\u001b[39m=\u001b[39;49minclude_cutoff_time\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhive-emr-8xlarge-1node/home/hadoop/DFS2SQL/yarn.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/featuretools/computational_backends/calculate_feature_matrix.py:221\u001b[0m, in \u001b[0;36mcalculate_feature_matrix\u001b[0;34m(features, entityset, cutoff_time, instance_ids, dataframes, relationships, cutoff_time_in_index, training_window, approximate, save_progress, verbose, chunk_size, n_jobs, dask_kwargs, progress_callback, include_cutoff_time)\u001b[0m\n\u001b[1;32m    218\u001b[0m     instance_ids \u001b[39m=\u001b[39m df[index_col]\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m is_instance(instance_ids, dd, \u001b[39m\"\u001b[39m\u001b[39mSeries\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     instance_ids \u001b[39m=\u001b[39m instance_ids\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m    222\u001b[0m \u001b[39melif\u001b[39;00m is_instance(instance_ids, ps, \u001b[39m\"\u001b[39m\u001b[39mSeries\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    223\u001b[0m     instance_ids \u001b[39m=\u001b[39m instance_ids\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/client.py:3224\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3222\u001b[0m         should_rejoin \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3224\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgather(packed, asynchronous\u001b[39m=\u001b[39;49masynchronous, direct\u001b[39m=\u001b[39;49mdirect)\n\u001b[1;32m   3225\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   3226\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m futures\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/client.py:2359\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2358\u001b[0m     local_worker \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msync(\n\u001b[1;32m   2360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gather,\n\u001b[1;32m   2361\u001b[0m     futures,\n\u001b[1;32m   2362\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   2363\u001b[0m     direct\u001b[39m=\u001b[39;49mdirect,\n\u001b[1;32m   2364\u001b[0m     local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m   2365\u001b[0m     asynchronous\u001b[39m=\u001b[39;49masynchronous,\n\u001b[1;32m   2366\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/utils.py:351\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[39mreturn\u001b[39;00m future\n\u001b[1;32m    350\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\n\u001b[1;32m    352\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, callback_timeout\u001b[39m=\u001b[39;49mcallback_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    353\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/utils.py:418\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[1;32m    417\u001b[0m     typ, exc, tb \u001b[39m=\u001b[39m error\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/utils.py:391\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m         future \u001b[39m=\u001b[39m wait_for(future, callback_timeout)\n\u001b[1;32m    390\u001b[0m     future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 391\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m future\n\u001b[1;32m    392\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m     error \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/tornado/gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         value \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    768\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    769\u001b[0m         \u001b[39m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[39m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[39m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m         exc: Optional[\u001b[39mException\u001b[39;00m] \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/miniconda/envs/dfs/lib/python3.8/site-packages/distributed/client.py:2222\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2220\u001b[0m         exc \u001b[39m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2221\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2222\u001b[0m         \u001b[39mraise\u001b[39;00m exception\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2223\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m   2224\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mskip\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/distributed/worker.py:2940\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/distributed/worker.py:2928\u001b[0m, in \u001b[0;36mloads_function\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/distributed/protocol/pickle.py:96\u001b[0m, in \u001b[0;36mloads\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/s3fs/__init__.py:1\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/s3fs/core.py:29\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/aiobotocore/session.py:1\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/botocore/translate.py:16\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/botocore/utils.py:37\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/botocore/httpsession.py:45\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/urllib3/contrib/pyopenssl.py:50\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/OpenSSL/__init__.py:8\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/OpenSSL/SSL.py:9\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/OpenSSL/_util.py:6\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:167\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:134\u001b[0m, in \u001b[0;36minit_static_locks\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:123\u001b[0m, in \u001b[0;36m_ensure_ffi_initialized\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt3/yarn/usercache/hadoop/appcache/application_1695350354974_0008/container_1695350354974_0008_01_000004/environment/lib/python3.8/site-packages/cryptography/hazmat/bindings/openssl/binding.py:43\u001b[0m, in \u001b[0;36m_legacy_provider_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: OpenSSL 3.0's legacy provider failed to load. This is a fatal error by default, but cryptography supports running without legacy algorithms by setting the environment variable CRYPTOGRAPHY_OPENSSL_NO_LEGACY. If you did not expect this error, you have likely made a mistake with your OpenSSL configuration."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "feature_matrix = calculate_feature_matrix(\n",
    "    features,\n",
    "    entityset=entityset,\n",
    "    cutoff_time=cutoff_time,\n",
    "    instance_ids=instance_ids,\n",
    "    training_window=training_window,\n",
    "    approximate=approximate,\n",
    "    cutoff_time_in_index=cutoff_time_in_index,\n",
    "    save_progress=save_progress,\n",
    "    chunk_size=chunk_size,\n",
    "    n_jobs=n_jobs,\n",
    "    dask_kwargs=dask_kwargs,\n",
    "    verbose=verbose,\n",
    "    progress_callback=progress_callback,\n",
    "    include_cutoff_time=include_cutoff_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result = feature_matrix.compute()\n",
    "result.to_parquet(\"./result.parquet\")\n",
    "end = time.time()\n",
    "print(f\"time: {end-start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfs",
   "language": "python",
   "name": "dfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
